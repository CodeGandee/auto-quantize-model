# Automatic Mixed-Precision Quantization for Vision Models

## Overview

Yes – there is an active research area dedicated to automatically choosing mixed precision for neural networks. This field is often referred to as **mixed-precision quantization** or **automatic bit-width allocation** in model compression. The core idea is to identify which layers (or even which parameters) are “sensitive” to quantization and assign those higher precision (e.g. 16-bit or 8-bit), while quantizing less sensitive parts to lower precision (8-bit, 4-bit, or even 2-bit). This approach applies broadly to all feed-forward vision models (CNNs, Vision Transformers like ViT, large detectors like YOLO, segmentation models like SAM, etc.), not just any one architecture. Researchers have shown that a judicious mixed-precision scheme can drastically reduce model size or speed up inference **without significant accuracy loss**, by avoiding the quantization of critical layers that would otherwise cause accuracy to drop[[1]](https://arxiv.org/abs/1905.03696#:~:text=to%20address%20these%20problems%20is,Here%2C%20we%20introduce%20Hessian%20AWare)[[2]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=sensitivities%20to%20quantization%2C%20and%20simple,architecture%20search%20or%20reinforcement%20learning).

## Why Mixed Precision and “Sensitive” Layers?

Not all layers of a network are equally robust to quantization. Some layers can be aggressively quantized to low bit-width (e.g. 4-bit or lower) with negligible effect on accuracy, while others—often early layers, final layers, or certain key blocks—are much more sensitive. Quantizing those sensitive layers causes disproportionate accuracy degradation. Fixed, uniform precision schemes (say, making every layer int8 or int4) often **over-quantize critical layers or under-utilize robustness of others**, leading to either accuracy loss or inefficiency[[3]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=provides%20significant%20computational%20and%20storage,Automatic%20search%20methods). Mixed-precision quantization addresses this by assigning different bit-widths to different layers to balance accuracy and efficiency[[2]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=sensitivities%20to%20quantization%2C%20and%20simple,architecture%20search%20or%20reinforcement%20learning). In practice, *“more sensitive layers are kept at higher precision”* while more robust layers use lower precision[[4]](https://ui.adsabs.harvard.edu/abs/2019arXiv191103852D/abstract#:~:text=A%20promising%20method%20to%20address,However%2C%20the%20search)[[5]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=which%20assign%20different%20bit%20widths,that%20layers%20with%20a%20higher). The challenge is that doing this selection by manual trial-and-error is indeed primitive and infeasible for large networks – the search space of per-layer bit assignments is exponential in the number of layers[[6]](https://arxiv.org/abs/1905.03696#:~:text=solution%20for%20this%20is%20to,selection%20of%20the%20relative%20quantization). Thus, a lot of research focuses on **automating the search for an optimal mixed-precision policy**[[1]](https://arxiv.org/abs/1905.03696#:~:text=to%20address%20these%20problems%20is,Here%2C%20we%20introduce%20Hessian%20AWare).

## Approaches for Automatic Mixed-Precision Quantization

Researchers have developed several strategies to automatically find which layers should remain higher precision. These can be broadly grouped into: (1) **sensitivity-based analytical methods**, (2) **search or learning-based methods**, and (3) **optimization formulation methods**. Many modern works combine elements of these. Below we outline the main approaches, especially highlighting recent advances.

### Sensitivity-Based Methods (Analytical Metrics)

One common approach is to measure a proxy for each layer’s quantization sensitivity, then allocate bit-widths accordingly. In these methods, the full-precision model is analyzed (often with a small calibration dataset) to quantify how much error each layer would introduce if quantized. Examples of sensitivity metrics include:

* **Hessian-based metrics:** The Hessian of the loss w.r.t. layer parameters can indicate sensitivity. **HAWQ (Hessian AWare Quantization)** introduced a systematic way to pick layer precisions based on the layer’s Hessian spectrum[[7]](https://arxiv.org/abs/1905.03696#:~:text=quantizing%20the%20model%20to%20a,on%20ResNet20%2C%20as%20compared%20to). In HAWQ, layers with large Hessian eigenvalues (indicating more impact on loss) are given higher precision, while more “stable” layers can be quantized more. HAWQ provided the first clear *automatic* criterion for mixed precision, and it showed that using second-order information yields near-optimal bit allocation without exhaustive search[[7]](https://arxiv.org/abs/1905.03696#:~:text=quantizing%20the%20model%20to%20a,on%20ResNet20%2C%20as%20compared%20to). Follow-up works **HAWQ-V2** and **HAWQ-V3** (2020–2021) refined this by using the average **Hessian trace** per layer as the sensitivity metric[[8]](https://www.nature.com/articles/s41598-025-91684-8#:~:text=Mixed%20precision%20quantization%20based%20on,significance%20of%20layers%20and), and by ensuring hardware-friendly integer-only implementation. These Hessian-aware methods essentially rank layers by estimated error impact; this often aligns with intuition (e.g. first and last layers tend to have high Hessian spectra, so they remain at 8-bit or 16-bit) and achieves a good accuracy-compression trade-off.
* **Activation or output metrics:** Another heuristic is to look at the distributions of activations/outputs. Some earlier works measured the Kullback-Leibler divergence between a layer’s full-precision output distribution and its quantized output distribution[[9]](https://ar5iv.org/html/2307.05657v2#:~:text=quantized%20layer,methods%20minimize%20total%20sum%20of) – a larger KL divergence means the layer doesn’t quantize nicely (more sensitive). More recently, an **information entropy** based method (Scientific Reports, 2025) was proposed: it computes the entropy of each layer’s output and uses that as a global importance indicator[[10]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=To%20address%20this%20issue%2C%20a,of%20each%20layer%20is%20properly)[[11]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=assessed%20from%20a%20global%20perspective,Conversely). The rationale is that a layer with high output entropy produces rich, information-dense features that require higher precision to avoid losing information, whereas a low-entropy (redundant or simpler) output can be represented with fewer bits. They adjust bit-widths based on output entropy thresholds and reported that this entropy-based allocation improved performance by protecting the truly critical layers[[11]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=assessed%20from%20a%20global%20perspective,Conversely). Notably, that work also integrated knowledge distillation (discussed later) to fine-tune the quantized model under a teacher’s guidance[[12]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=layers%20with%20lower%20entropy%20that,accuracy%20of%20the%20quantized%20model).
* **Layerwise quantization sensitivity via perturbation:** Some practical approaches simply try *quantizing one layer at a time* (to a lower bit) and see how it affects the model’s accuracy or loss on a calibration set – essentially measuring sensitivity by direct experiment. While brute-force across all layers is expensive, methods like **InfoQ** (2021) streamline this by a single forward pass: they pass data through the network while simulating each layer at various bit-widths one by one, and measure a drop in some accuracy proxy or output similarity[[13]](https://www.researchgate.net/publication/394396788_InfoQ_Mixed-Precision_Quantization_via_Global_Information_Flow#:~:text=InfoQ%3A%20Mixed,forward%20pass%2C%20the%20resulting). This gives a quick ranking of which layers cause the most damage when quantized. Those layers are then kept at higher precision. This empirical sensitivity test approach is conceptually simple and does not require labels (just forward passes), making it attractive for post-training quantization scenarios.

In summary, sensitivity-based methods treat the layer selection as a **greedy allocation based on metrics**. They are fast since they avoid searching over combinations: you compute a score per layer and then choose bit-widths (often by sorting scores and meeting a hardware budget). However, a known limitation is that they **assume layer quantization effects are independent** and simply add up – which isn’t always true[[14]](https://ar5iv.org/html/2307.05657v2#:~:text=A%20critical%20limitation%20of%20existing,on%20the%20assumption%20of%20independence). Quantization errors can interact (e.g., two layers both low-precision might jointly cause a bigger error than the sum of individually). Newer methods like CLADO (below) address this by considering interactions, but pure sensitivity metrics remain a popular starting point because of their simplicity and low data requirement.

### Search-Based Methods (NAS and Reinforcement Learning)

Another major direction is to pose mixed-precision selection as a **search problem** and use techniques from neural architecture search (NAS) or reinforcement learning (RL) to explore it. Instead of relying on a closed-form metric, these methods try different bit-width configurations and evaluate the resulting model (or use a proxy for evaluation), then use an algorithm to converge on a good configuration.

* **Reinforcement Learning (RL):** The prototypical example is **HAQ** by Wang et al. (CVPR 2019). HAQ uses an RL agent (controller) to decide bit-widths for each layer, treating it like a sequential decision process[[15]](https://arxiv.org/abs/1811.08886#:~:text=In%20this%20paper%2C%20we%20introduce,Our%20framework%20effectively). The agent gets a reward based on the final model’s accuracy (and can also incorporate latency or energy into the reward). Through many trial configurations (simulations of quantized model accuracy), the RL agent learns a policy that maps a layer’s attributes to an optimal bit-width. The key benefit is that RL can handle multiple objectives – HAQ was **hardware-aware**, using a hardware simulator to get latency/energy for a given bit allocation, and then finding a policy that met resource targets while maximizing accuracy[[16]](https://arxiv.org/abs/1811.08886#:~:text=In%20this%20paper%2C%20we%20introduce,9x%20with). The end result was a specialized quantization scheme per layer that outperformed uniform 8-bit in latency by 1.5×–2× with negligible accuracy loss[[17]](https://arxiv.org/abs/1811.08886#:~:text=we%20employ%20a%20hardware%20simulator,are%20drastically). This approach removed the manual guesswork and was *fully automated*, at the cost of substantial search time (training the RL and evaluating many configs). Other researchers have also applied evolutionary algorithms or even simple brute-force search with early stopping to this problem, but RL (and also **Bayesian optimization**) became popular for mixed precision. For instance, a 2023 method uses a **Tree-structured Parzen Estimator (TPE)** to guide search: it narrows down promising bit assignments via Bayesian optimization, achieving a 20% model size reduction without accuracy loss and with much fewer trials than brute force[[18]](https://arxiv.org/abs/2308.06422#:~:text=becomes%20paramount,Through%20rigorous%20testing%20on%20well).
* **Differentiable NAS:** Instead of treating bit choices as discrete actions, differentiable NAS methods relax bit-width decisions to continuous parameters that can be optimized with gradient descent. An early work by Wu et al. (2018) formulated mixed-precision selection as part of a DNAS (Differentiable Neural Architecture Search) framework[[19]](https://arxiv.org/abs/1812.00090#:~:text=,DNAS%29%20framework). They added learnable “switches” or coefficients for different bit-width options in each layer and introduced a differentiable approximation so that standard backpropagation could tune those coefficients. After training, the highest-weight option for each layer is chosen (e.g., whether the layer ends up 4-bit or 8-bit is determined by the learned continuous variables). This approach finds a policy much faster than RL since it uses gradients, though it requires a training process and careful regularization to push the network to a valid discrete solution. Differentiable search has also been combined with distillation and other tricks in later works, and some recent NAS frameworks (like FBNet and others) incorporate bit-width as part of the architecture search.
* **Evolutionary and other search:** Some works use evolutionary algorithms to mutate and evolve bit-width assignments with an accuracy fitness metric. Others simply do an exhaustive or random search combined with early stopping. These search methods can directly evaluate *actual accuracy* of a quantized model on a validation set for each candidate, which is very direct but can be extremely expensive (because the search space is huge). To mitigate that, most researchers either use proxy metrics or limit the search via sensitivity (hybrid approach). Overall, pure search is less common now than it was initially, due to the high computational cost (many hundreds of evaluations – as noted, search-based methods can take **hundreds or thousands of GPU-hours** without careful optimization[[20]](https://ar5iv.org/html/2307.05657v2#:~:text=based%20methods%20Wang%20et%C2%A0al,%282022%29%2C%20on)).

**Recent example:** A novel work **ARQ (2024)** combines reinforcement learning with a *robustness* objective – it searches for bit-width configurations that preserve not only accuracy but also adversarial robustness of the model[[21]](https://arxiv.org/html/2410.24214v1#:~:text=This%20paper%20introduces%20ARQ%2C%20an,better%20than%20these%20baselines%20across)[[22]](https://arxiv.org/html/2410.24214v1#:~:text=that%20not%20only%20preserves%20the,In%20many%20cases%2C%20the). This indicates how flexible the search formulation is: one can plug in different goals (latency, robustness, etc.) alongside accuracy. ARQ’s RL agent, for instance, was guided by a robustness certification method to ensure the chosen mixed precision policy didn’t overly compromise the model’s resilience to input perturbations[[23]](https://arxiv.org/html/2410.24214v1#:~:text=maintains%20their%20certified%20robustness,of%20the%20original%20DNN%20with)[[24]](https://arxiv.org/html/2410.24214v1#:~:text=quantization%20policies%20%E2%80%93%20the%20bit,grained%20control%20over%20the). It outperformed earlier RL-based schemes (like HAQ) on both accuracy and certified robustness[[25]](https://arxiv.org/html/2410.24214v1#:~:text=quantization%20methods%20that%20only%20optimize,ImageNet%2C%20and%20MobileNetV2%20on%20ImageNet). This is a cutting-edge example showing the field’s evolution – moving beyond just accuracy/speed trade-off to other considerations – but the underlying theme remains using automated search to replace manual precision tuning.

### Optimization-Based Formulation Methods

More recently, researchers have started formulating the mixed-precision selection as an explicit optimization or solver problem. If one can derive a good proxy for the accuracy as a function of layer bit-widths, then selecting the bits becomes an optimization problem (often constrained by a hardware budget like total model size or BitOps). Two notable approaches in this category:

* **Integer Linear/Quadratic Programming (ILP/IQP):** Chen Tang *et al.* (ECCV 2022) introduced a method where they first perform a **quantization-aware training** (QAT) on the full model but allow *learnable scale factors* in each layer’s quantizer[[26]](https://arxiv.org/abs/2203.08368#:~:text=layer,we%20propose%20a%20joint%20training). These scale factors (which essentially determine quantization step size for that layer) turn out to correlate with the layer’s importance/sensitivity – intuitively, if during QAT a layer’s quantizer ends up with a very tight scale (small step), it means the layer had to preserve more information (was sensitive). They showed these learned scale factors can serve as *importance indicators* for each layer[[27]](https://arxiv.org/abs/2203.08368#:~:text=reveal%20that%20some%20unique%20learnable,parallelizing%20the%20original%20sequential%20training). The clever part is they devised a joint training scheme to learn all layer indicators in one pass (rather than layer-by-layer), making it efficient[[28]](https://arxiv.org/abs/2203.08368#:~:text=contains%20hundreds%20of%20such%20indicators%2C,on%20ResNet18%20with%20our%20indicators). Once they have an importance value for each layer, they formulate the bit-width assignment as a **one-shot optimization problem**: they set up an Integer Linear Program that maximizes a proxy for accuracy (or minimizes total error) subject to constraints like “sum of (bits \* layer\_size) <= budget”[[29]](https://arxiv.org/abs/2203.08368#:~:text=scheme%20that%20can%20obtain%20all,ranging%20models%20with%20various). They can then solve this ILP very quickly (in fractions of a second for ResNet-18) to find the optimal mix of 4-bit, 6-bit, 8-bit, etc., per layer[[30]](https://arxiv.org/abs/2203.08368#:~:text=processes,available%20on%20this%20https%20URL). This method, which we can call **LIMPQ (Learned Importance for Mixed Precision Quantization)**, achieved state-of-the-art accuracy on ImageNet for multiple networks while being orders of magnitude faster in search time than iterative methods[[30]](https://arxiv.org/abs/2203.08368#:~:text=processes,available%20on%20this%20https%20URL). It essentially bypasses the need for trial-and-error by turning the search into a mathematical programming problem. An updated version by Zihao Deng *et al.* (arXiv 2023) called **CLADO** goes a step further: it captures **cross-layer dependency** by including pairwise error terms in the cost function[[31]](https://ar5iv.org/html/2307.05657v2#:~:text=We%20propose%20the%20first%20sensitivity,can%20be%20solved%20within%20seconds). CLADO uses a small calibration set to compute how quantizing any pair of layers together affects the loss (solving a system of linear equations to approximate these interactions)[[31]](https://ar5iv.org/html/2307.05657v2#:~:text=We%20propose%20the%20first%20sensitivity,can%20be%20solved%20within%20seconds). Then it expresses the total quantization error as a quadratic function of the bit allocation and solves an **Integer Quadratic Program** to minimize error under a model size constraint. Because it explicitly models interactions, it finds bit allocations that earlier independent methods miss, yielding higher accuracy for the same size. The authors report that CLADO outperforms prior mixed-precision schemes, especially at aggressive compression (e.g. when many layers must go to 4-bit)[[32]](https://ar5iv.org/html/2307.05657v2#:~:text=of%20the%20sensitivity%20set%20and,are%20aggressively%20compressed%20to%20low)[[33]](https://ar5iv.org/html/2307.05657v2#:~:text=Figure%206%3A%20Bit,50). Despite the complexity of modeling layer interactions, their IQP solver still runs in seconds, making it quite practical. (Code for CLADO was released, indicating its viability in real scenarios[[34]](https://arxiv.org/abs/2307.05657#:~:text=based%20MPQ%20algorithm%20that%20captures,available%20here%3A%20this%20https%20URL).)
* **Hybrid Analytical+Search:** Some approaches combine analytical modeling with search to reduce cost. For example, the *Sensitivity-Aware NAS* by Azizi et al. (arXiv 2023) first prunes away less important channels via Hessian-based pruning to simplify the network, then uses a Bayesian optimization to search for the best combination of layer widths (pruning) and precisions[[18]](https://arxiv.org/abs/2308.06422#:~:text=becomes%20paramount,Through%20rigorous%20testing%20on%20well). By pruning and clustering similar layers, they shrink the search space and then efficiently zero in on an optimal design (they reported a 12× reduction in search time compared to naive methods)[[35]](https://arxiv.org/abs/2308.06422#:~:text=based%20pruning%2C%20ensuring%20the%20removal,focused). This is indicative of a trend: combining **second-order analysis (Hessian)** to guide or constrain the search, and then using an **automated search algorithm** for the final selection. The result is a more tractable optimization that still considers a wide space of possibilities.

In short, optimization-based methods attempt to “solve” the mixed-precision assignment in a single shot (or at least more directly than trial-and-error). These methods are quite new and promising, as they often find better solutions by considering global effects (like cross-layer error) that simpler methods ignore. They also tend to be data-efficient – many rely only on a small calibration set or a few forward passes to estimate errors, rather than needing to train and retrain multiple models.

### Other Noteworthy Criteria and Methods

Beyond the main categories above, there are some miscellaneous advancements worth mentioning:

* **Knowledge Distillation as a criterion:** In quantization-aware training, sometimes the sensitivity of a layer is measured by how much that layer’s quantization affects the *output logits or teacher divergence*. Some works allocate precision to minimize the KL divergence between the student (quantized network) and teacher (full network) outputs. This effectively uses the teacher model’s predictions as a guide for where the student is struggling due to quantization. Layers that cause the student to diverge from teacher outputs are prioritized for higher precision.
* **Group-wise or Channel-wise mixed precision:** While most research optimizes precision per layer, a few explore even finer granularity – e.g., not every channel or neuron in a layer needs the same precision. This becomes a much larger search space, but techniques like grouping channels by sensitivity and assigning mixed precision within a layer have been studied (though not yet common in practice due to hardware limitations). These approaches often rely on similar sensitivity metrics (e.g., some channels have outlier activation distributions and need more bits). This is an emerging area but currently layer-wise precision is the standard unit because it aligns with how hardware typically quantizes whole tensor operations.
* **“Sensitive weights” vs sensitive layers:** You mentioned sensitive *weights* as well – indeed, certain weight tensors (like the kernel of a critical convolution) might need higher precision. Most methods treat an entire weight tensor as one unit for bit assignment. However, there are ideas like **per-tensor vs per-vector precision** in some hardware (e.g., one can quantize different weight matrices in an MLP to different bits). Some of the above methods can be applied at that granularity as well.
* **Open-loop vs closed-loop evaluation:** Some approaches (especially older ones) picked precisions based on a proxy (like Hessian or SNR) without actually testing the quantized model until the end. Newer “accuracy-aware” methods often include a feedback loop: they quantize the model with a candidate scheme and **evaluate on a small validation set** to get actual accuracy, then adjust. This can be done in a guided way (as Intel’s tool does, next section) to ensure the final scheme meets a desired accuracy drop threshold.

## Post-Training (Data-Free) vs. Quantization-Aware Training

You specifically asked about methods that do **not need labeled data (or only a little)**, possibly using the original model as a teacher. This is an important practical consideration: if we want to quantize a model without full re-training, we often rely on a small calibration dataset and/or the teacher model’s knowledge to avoid requiring the original training labels. Both Post-Training Quantization (PTQ) and QAT paradigms have solutions for this:

### Post-Training Quantization with Minimal Data

**PTQ** methods assume you have a pretrained full-precision model and possibly a tiny unlabeled dataset (for calibration). The goal is to adjust quantization scales or even weights *without gradient descent on ground-truth labels*. Automatic mixed precision fits well here: you can use the full model to evaluate layer sensitivity with just a few forward passes. Many of the sensitivity-based methods (Hessian, entropy, etc.) fall in this category – they only need forward prop on some data to decide bit widths. Once the bit-widths are chosen, PTQ typically applies some calibration to set optimal quantization parameters (min/max or scale/zero-point) and might do minor weight adjustments (like bias correction).

In recent years, PTQ has become surprisingly powerful with advanced techniques: **BRECQ (ICLR 2021)** is a prime example of a *label-free* post-training method that achieves near-QAT accuracy even at 4-bit or lower. BRECQ works by **reconstructing layer outputs**: it takes small batches of unlabeled data, passes them through the original model and the quantized model, and then optimizes the quantized model’s weights to minimize the output difference for each block or layer[[36]](https://openreview.net/forum?id=POWv6hDd9XH#:~:text=PTQ%20framework%2C%20dubbed%20BRECQ%2C%20which,architectures%20are%20conducted%20for%20both). Essentially, the full-precision model acts as a teacher, providing target activations that the quantized network should match. This is done layer by layer (or block by block), which is a form of knowledge distillation at the layer level. Because no labeled loss is needed – just the teacher’s outputs – BRECQ and similar methods (like AdaRound, which optimizes the rounding of each weight tensor using a small data batch and a loss on output mismatch) can operate with *only a few hundred unlabeled samples*. They have demonstrated that even **2-bit** weights can be achieved on some networks with minimal accuracy drop using these techniques[[37]](https://openreview.net/forum?id=POWv6hDd9XH#:~:text=requires%20a%20small%20subset%20of,layer%20sensitivity.%20Extensive%20experiments)[[38]](https://openreview.net/forum?id=POWv6hDd9XH#:~:text=image%20classification%20and%20object%20detection,com%2Fyhhhli%2FBRECQ). For mixed precision specifically, BRECQ incorporates a step to decide which layers go to lower bit-widths by approximating inter-layer sensitivity (in their work, they mention incorporating mixed precision by looking at intra- vs inter-block sensitivity as well)[[39]](https://openreview.net/forum?id=POWv6hDd9XH#:~:text=networks%20and%20reconstructs%20them%20one,240%20times%20faster%20production%20of). The outcome is that PTQ is no longer limited to 8-bit: it can produce highly compressed models and it inherently uses the FP model as the teacher (since it’s matching its outputs).

**Data-free quantization** is another sub-topic: even if you have *no* real calibration data, techniques exist to generate surrogate data. **ZeroQ (CVPR 2020)**, for example, generates fake images by optimizing random noise to match the batch statistics (mean/variance) of the FP model’s layers, then uses those as calibration data for PTQ. This can be combined with mixed-precision selection as well – e.g., use the fake data to measure layer sensitivity. There are also GAN-based approaches to synthesize inputs that maximize activation coverage. These allow completely data-free operation, which is useful for proprietary models or scenarios where the training data isn’t accessible. The trade-off is that the synthetic data must be good enough to approximate real inputs; when it is, data-free PTQ can achieve results close to having a few real samples.

In summary, **PTQ with teacher guidance** means we rely on the pretrained model’s behavior (statistics or outputs) to drive quantization. It typically doesn’t require labels at all. Many of the automated mixed-precision algorithms (HAWQ, CLADO, etc.) are actually demonstrated in a PTQ context: they choose the bit allocation based on the FP model analysis, then you do calibration and perhaps some fine-tuning like BRECQ to finalize the quantized model. This addresses the case where you cannot do a full retraining on a labeled dataset.

### Quantization-Aware Training (QAT) with Knowledge Distillation

If you *do* have some training data (even if unlabeled), a more accurate approach is QAT, where you fine-tune the model with quantization simulated in the forward pass. Traditionally, QAT requires the training dataset (with labels) to update weights and recover any accuracy lost from quantization. However, as you noted, it’s possible to perform QAT in a teacher-student fashion using the original model, which reduces the need for true labels. Essentially, this becomes a form of **knowledge distillation (KD)**: during QAT, you take the output logits or feature maps of the full-precision teacher model as a target for the quantized student model. The loss can be a combination of the usual training loss (if labels are available) and a distillation loss (teacher outputs vs student outputs). In extreme cases, if no labels are available, you can run QAT purely with a distillation loss – feeding in either real unlabeled samples or even generated samples, having the teacher produce “soft targets,” and training the quantized network to match those. This is sometimes called *zero-shot quantization aware training*. For example, a 2021 work on **Self-Supervised Quantization-Aware KD (SQAKD)** demonstrates QAT where the student learns from the teacher’s predictions and some self-supervised consistency, without any ground truth labels[[40]](https://proceedings.mlr.press/v238/zhao24d/zhao24d.pdf#:~:text=%5BPDF%5D%20Self,manner%20without%20labeled%20data)[[41]](https://proceedings.mlr.press/v238/zhao24d/zhao24d.pdf#:~:text=Second%2C%20we%20propose%20a%20Self,manner%20without%20labeled%20data). The student essentially tries to mimic the teacher’s function. This approach can be highly effective in maintaining accuracy: the teacher’s knowledge ensures the student doesn’t drift too far.

Even when labels are available, using the teacher model in QAT is a **best practice for ultra-low precision**. Many 4-bit or 2-bit QAT recipes (including those from industry, like NVIDIA’s or Qualcomm’s toolkits) include a KD term to stabilize training. It’s particularly helpful because the quantized model has a smaller capacity (due to reduced precision) and may not otherwise learn the finer details; the teacher provides a gentle learning signal. So, to answer your question: **yes**, there are methods to fine-tune a quantized model with minimal or no labeled data by leveraging the original model’s outputs as supervision.

In terms of automation: one can combine this with the automatic layer selection. For instance, an RL search or an ILP solution could propose a mixed-precision bit allocation, and then you perform a short QAT (few epochs) with distillation to recover any small accuracy gap. This two-step approach (search then distill-finetune) is often used in recent research to get the best of both worlds – automatic design and training recovery. The reference that used entropy also explicitly mentioned integrating knowledge distillation to **mitigate accuracy loss** in their mixed-precision scheme[[12]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=layers%20with%20lower%20entropy%20that,accuracy%20of%20the%20quantized%20model). Overall, QAT with KD is a powerful method to squeeze out extra performance, and it naturally uses the full-precision model as a teacher, aligning perfectly with your interest in techniques that don’t require fresh labels.

## Recent Advances and Open-Source Tools

**Prioritize recent methods:** In the last 2-3 years, many new mixed-precision techniques have emerged. We’ve already highlighted some 2022–2024 works like LIMPQ (Tang 2022), CLADO (Deng et al. 2023), the entropy-based method (2025), ARQ (Yang et al. 2024 for robustness). These represent the state-of-the-art in research. They aim to make the process more efficient (e.g., solving in seconds, as LIMPQ does[[30]](https://arxiv.org/abs/2203.08368#:~:text=processes,available%20on%20this%20https%20URL)) and more effective (maintaining accuracy even at 4-bit or with additional robustness constraints[[25]](https://arxiv.org/html/2410.24214v1#:~:text=quantization%20methods%20that%20only%20optimize,ImageNet%2C%20and%20MobileNetV2%20on%20ImageNet)). The trend is also toward handling Transformers and large vision models (not just CNNs). For example, CLADO reports results on both CNNs and a Vision Transformer on ImageNet, showing its generality[[34]](https://arxiv.org/abs/2307.05657#:~:text=based%20MPQ%20algorithm%20that%20captures,available%20here%3A%20this%20https%20URL). Another 2023 paper by Hongyu Cai et al. (if we note one) introduced **MixQuant** for ViTs, which deals with the fact that transformer layers may have different sensitivities (e.g., attention vs MLP blocks). In general, modern methods are *model-agnostic*: they can be applied to ViT, SAM, YOLO, etc., treating them as a collection of layers to optimize. So you can be confident that the field’s findings are not limited to small networks – they are being scaled up to handle the kinds of large vision models you mentioned.

**Battle-tested open-source projects:** Importantly, many of these techniques are no longer confined to paper – they’re available in libraries or tools:

* **Intel Neural Compressor (INC):** This is an open-source Python library that implements many compression techniques (quantization, pruning, distillation). For quantization, INC has an *accuracy-driven mixed-precision tuning mode*. In practice, you can give it a model and a calibration or validation dataset, and it will **automatically search through mixed-precision configurations** to meet a desired accuracy threshold[[42]](https://www.marktechpost.com/2022/07/18/meet-intel-neural-compressor-an-open-source-python-library-for-model-compression-that-reduces-the-model-size-and-increases-the-speed-of-deep-learning-inference-for-deployment-on-cpus-or-gpus/#:~:text=In%20order%20to%20allow%20fine,The%20tool%20searches%20the)[[43]](https://www.marktechpost.com/2022/07/18/meet-intel-neural-compressor-an-open-source-python-library-for-model-compression-that-reduces-the-model-size-and-increases-the-speed-of-deep-learning-inference-for-deployment-on-cpus-or-gpus/#:~:text=adjustment%20method%20for%20increased%20quantization,reaches%20the%20accuracy%20objective%20and). It essentially does what you described manually: it will try quantizing everything to int8, measure accuracy; if accuracy drops too much, it will smartly keep certain layers in higher precision (fp16 or fp32) and try again, iterating until the accuracy is acceptable. This “automatic accuracy-aware tweaking” is a built-in feature[[44]](https://www.marktechpost.com/2022/07/18/meet-intel-neural-compressor-an-open-source-python-library-for-model-compression-that-reduces-the-model-size-and-increases-the-speed-of-deep-learning-inference-for-deployment-on-cpus-or-gpus/#:~:text=learning%20frameworks,including%20TensorFlow%2C%20PyTorch%2C%20and%20MXNet)[[45]](https://www.marktechpost.com/2022/07/18/meet-intel-neural-compressor-an-open-source-python-library-for-model-compression-that-reduces-the-model-size-and-increases-the-speed-of-deep-learning-inference-for-deployment-on-cpus-or-gpus/#:~:text=quantization%2C%20automatic%20mixed%20precision%2C%20and,tuning%20space%20of%20various%20quantization). Under the hood, it uses techniques like layer sensitivity ranking and incremental fallback (e.g., start with all int8, then raise precision of the worst-offending layer to fp16, and so on). INC’s approach is informed by research, and it’s considered *battle-tested* because it’s used in industry for quantizing models on Intel hardware. It supports popular frameworks (TensorFlow, PyTorch, etc.) and can handle vision models like ResNet, YOLO, and even transformers. Users have reported good results with minimal manual intervention – exactly the convenience one would want.
* **NVIDIA TensorRT and others:** Nvidia’s deployment stack (TensorRT) supports mixed precision (typically FP16/INT8). While TensorRT doesn’t automatically decide per-layer precision (developers often specify which layers to quantize to int8 and which to leave in FP16), Nvidia has published guidelines and tooling for this. Their approach often relies on calibration and measuring per-layer impact. For instance, they recommend keeping the first convolution and last fully-connected layer in higher precision if int8 calibration shows a significant loss. Nvidia’s QAT toolkit (like in TensorFlow Quantization Toolkit or PyTorch QAT examples) frequently uses **knowledge distillation** during fine-tuning and allows you to manually set different bit-widths per layer – albeit manual, these tools are built on the findings that some layers need higher precision. We might soon see more automated support from Nvidia, but much of it is currently semi-manual but guided by well-known best practices from research.
* **OpenVINO + NNCF:** OpenVINO (Intel’s inference engine) paired with the Neural Network Compression Framework (NNCF) also has an “Accuracy-aware quantization” pipeline[[46]](https://medium.com/openvino-toolkit/enhanced-low-precision-pipeline-to-accelerate-inference-with-openvino-toolkit-deefe0206c24#:~:text=,while%20maintaining%20a%20predefined). This pipeline will quantize a model and if the accuracy drops beyond a threshold, it will automatically restore certain layers to higher precision and retry (or adjust quantization parameters), in a loop, until it finds a mix that preserves accuracy within the threshold. It’s essentially an automated search with the constraint “accuracy loss < X%”. This is very useful for real-world deployments because you can say, for example, “give me the fastest int8 model that stays within 1% of original accuracy” – the tool will then do the exploration for you.
* **Academic code releases:** Many research papers release their code, which can be leveraged. For example, the **HAQ** code (from MIT Han Lab) is on GitHub and can be used to replicate the RL-based bit search[[47]](https://github.com/mit-han-lab/haq#:~:text=HAQ%3A%20Hardware,CVPR2019%2C%20oral). The HAWQ authors released a PyTorch quantization library that implements their Hessian-based mixed precision (with support for TVM backend)[[48]](https://github.com/Zhen-Dong/HAWQ#:~:text=Zhen,V3). Tang et al.’s ILP method (published as **MixPrec** in ECCV 2022) also has open-source code[[30]](https://arxiv.org/abs/2203.08368#:~:text=processes,available%20on%20this%20https%20URL). **BRECQ** code is on GitHub as well, and has become a popular benchmark for PTQ. What this means is that if you are looking to apply these ideas, you don’t always have to re-implement from scratch – you can often find an existing implementation or even a ready-to-use tool.

To give a concrete example, if you have a model like YOLOv10 and you want to quantize with minimal hassle: you could use Intel Neural Compressor or OpenVINO’s accuracy-aware quantization to automatically find a mixed-precision int8/fp16 strategy. Under the hood, these will likely end up keeping some layers (perhaps certain layers in the detection head or early layers processing the image) in higher precision based on their sensitivity. This automated fallback is directly informed by the research we discussed (like layer sensitivity metrics). On the other hand, if you wanted to experiment with more cutting-edge approaches, you could look at CLADO or LIMPQ code to see how they decide bit-widths, and apply that to your model (these might require a bit of coding and a calibration set, but they are available).

In terms of **recentness**, the field is moving fast. Publications in 2023 and 2024 (e.g., from arXiv and conferences like ICML, NeurIPS) have focused on making mixed-precision more *efficient* (faster search, fewer assumptions). The fact that open-source tools have already adopted “accuracy-aware” mixed precision (since ~2021–2022) indicates that some of the early research (2018–2020 like HAQ, HAWQ) has matured into practice. Newer ideas like entropy-based metrics or robust quantization (ARQ) are still in the research phase but could soon make their way into tools if they prove beneficial.

## Summary

In summary, there is a robust research field dedicated to automating mixed-precision quantization for deep vision models. Techniques exist to automatically identify “sensitive” layers that should remain in higher precision and to systematically explore combinations of precisions across the network instead of relying on manual trial and error. This can be seen as part of the broader model compression and NAS research domains, often specifically termed **mixed-precision neural network quantization**[[5]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=which%20assign%20different%20bit%20widths,that%20layers%20with%20a%20higher). Modern methods include:

* **Sensitivity-based heuristics** (e.g. Hessian-aware methods like HAWQ that assign precision based on second-order gradients[[7]](https://arxiv.org/abs/1905.03696#:~:text=quantizing%20the%20model%20to%20a,on%20ResNet20%2C%20as%20compared%20to), or entropy/KL metrics per layer),
* **Automated search algorithms** (from reinforcement learning in HAQ[[15]](https://arxiv.org/abs/1811.08886#:~:text=In%20this%20paper%2C%20we%20introduce,Our%20framework%20effectively) to differentiable NAS[[19]](https://arxiv.org/abs/1812.00090#:~:text=,DNAS%29%20framework) and Bayesian optimization),
* **Optimization solvers** that treat bit allocation as a mathematical programming problem (as in LIMPQ/CLADO, which solve for the best per-layer bit-width under constraints in one shot[[29]](https://arxiv.org/abs/2203.08368#:~:text=scheme%20that%20can%20obtain%20all,ranging%20models%20with%20various)).

These approaches significantly outperform naive manual selection, finding smaller/faster models while preserving accuracy. Many have been demonstrated on a variety of vision models (classification, detection, transformers) – for example, mixed-precision ResNets, MobileNets, and ViTs on ImageNet with minimal accuracy drop are reported in recent literature[[34]](https://arxiv.org/abs/2307.05657#:~:text=based%20MPQ%20algorithm%20that%20captures,available%20here%3A%20this%20https%20URL).

Crucially, if retraining data is limited, methods are available to fine-tune or calibrate the quantized model without labeled data: by **using the original model as a teacher**, either through direct layer output matching (PTQ like BRECQ) or through knowledge distillation during QAT[[12]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=layers%20with%20lower%20entropy%20that,accuracy%20of%20the%20quantized%20model). This means you can achieve excellent quantization results with only a small calibration set or even synthetic/unlabeled data – the full-precision model’s predictions guide the process.

Finally, what makes this field practically relevant is that its innovations have been **implemented in open-source frameworks and tools**. Whether through research code (many papers provide code for reproducibility) or through industry libraries (Intel Neural Compressor, OpenVINO, etc., offering accuracy-driven mixed precision tuning[[42]](https://www.marktechpost.com/2022/07/18/meet-intel-neural-compressor-an-open-source-python-library-for-model-compression-that-reduces-the-model-size-and-increases-the-speed-of-deep-learning-inference-for-deployment-on-cpus-or-gpus/#:~:text=In%20order%20to%20allow%20fine,The%20tool%20searches%20the)), you can leverage these automated strategies today. In essence, instead of manually selecting layers to keep in FP16, you can let these algorithms figure it out: they will automatically preserve the layers most critical to accuracy and quantize others more aggressively. By optimizing the layer selection problem with advanced methods, one can obtain quantized vision models that are as accurate as the original (or within an acceptable drop) while maximizing the efficiency gains – all with much less manual effort than before. The research and tools in automatic mixed-precision quantization are exactly aimed at solving the problem you described, and they have made a lot of progress in recent years[[15]](https://arxiv.org/abs/1811.08886#:~:text=In%20this%20paper%2C%20we%20introduce,Our%20framework%20effectively)[[34]](https://arxiv.org/abs/2307.05657#:~:text=based%20MPQ%20algorithm%20that%20captures,available%20here%3A%20this%20https%20URL).

[[1]](https://arxiv.org/abs/1905.03696#:~:text=to%20address%20these%20problems%20is,Here%2C%20we%20introduce%20Hessian%20AWare) [[6]](https://arxiv.org/abs/1905.03696#:~:text=solution%20for%20this%20is%20to,selection%20of%20the%20relative%20quantization) [[7]](https://arxiv.org/abs/1905.03696#:~:text=quantizing%20the%20model%20to%20a,on%20ResNet20%2C%20as%20compared%20to) [1905.03696] HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision

<https://arxiv.org/abs/1905.03696>

[[2]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=sensitivities%20to%20quantization%2C%20and%20simple,architecture%20search%20or%20reinforcement%20learning) [[3]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=provides%20significant%20computational%20and%20storage,Automatic%20search%20methods) [[5]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=which%20assign%20different%20bit%20widths,that%20layers%20with%20a%20higher) [[10]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=To%20address%20this%20issue%2C%20a,of%20each%20layer%20is%20properly) [[11]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=assessed%20from%20a%20global%20perspective,Conversely) [[12]](https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56#:~:text=layers%20with%20lower%20entropy%20that,accuracy%20of%20the%20quantized%20model) Mixed precision quantization based on information entropy | Scientific Reports

<https://www.nature.com/articles/s41598-025-91684-8?error=cookies_not_supported&code=a2f3a416-39b7-41d4-adc6-5f9aefd74c56>

[[4]](https://ui.adsabs.harvard.edu/abs/2019arXiv191103852D/abstract#:~:text=A%20promising%20method%20to%20address,However%2C%20the%20search) Hessian Aware trace-Weighted Quantization of Neural Networks - ADS

<https://ui.adsabs.harvard.edu/abs/2019arXiv191103852D/abstract>

[[8]](https://www.nature.com/articles/s41598-025-91684-8#:~:text=Mixed%20precision%20quantization%20based%20on,significance%20of%20layers%20and) Mixed precision quantization based on information entropy - Nature

<https://www.nature.com/articles/s41598-025-91684-8>

[[9]](https://ar5iv.org/html/2307.05657v2#:~:text=quantized%20layer,methods%20minimize%20total%20sum%20of) [[14]](https://ar5iv.org/html/2307.05657v2#:~:text=A%20critical%20limitation%20of%20existing,on%20the%20assumption%20of%20independence) [[20]](https://ar5iv.org/html/2307.05657v2#:~:text=based%20methods%20Wang%20et%C2%A0al,%282022%29%2C%20on) [[31]](https://ar5iv.org/html/2307.05657v2#:~:text=We%20propose%20the%20first%20sensitivity,can%20be%20solved%20within%20seconds) [[32]](https://ar5iv.org/html/2307.05657v2#:~:text=of%20the%20sensitivity%20set%20and,are%20aggressively%20compressed%20to%20low) [[33]](https://ar5iv.org/html/2307.05657v2#:~:text=Figure%206%3A%20Bit,50) [2307.05657] Mixed-Precision Quantization with Cross-Layer Dependencies

<https://ar5iv.org/html/2307.05657v2>

[[13]](https://www.researchgate.net/publication/394396788_InfoQ_Mixed-Precision_Quantization_via_Global_Information_Flow#:~:text=InfoQ%3A%20Mixed,forward%20pass%2C%20the%20resulting) InfoQ: Mixed-Precision Quantization via Global Information Flow

<https://www.researchgate.net/publication/394396788_InfoQ_Mixed-Precision_Quantization_via_Global_Information_Flow>

[[15]](https://arxiv.org/abs/1811.08886#:~:text=In%20this%20paper%2C%20we%20introduce,Our%20framework%20effectively) [[16]](https://arxiv.org/abs/1811.08886#:~:text=In%20this%20paper%2C%20we%20introduce,9x%20with) [[17]](https://arxiv.org/abs/1811.08886#:~:text=we%20employ%20a%20hardware%20simulator,are%20drastically) [1811.08886] HAQ: Hardware-Aware Automated Quantization with Mixed Precision

<https://arxiv.org/abs/1811.08886>

[[18]](https://arxiv.org/abs/2308.06422#:~:text=becomes%20paramount,Through%20rigorous%20testing%20on%20well) [[35]](https://arxiv.org/abs/2308.06422#:~:text=based%20pruning%2C%20ensuring%20the%20removal,focused) [2308.06422] Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation

<https://arxiv.org/abs/2308.06422>

[[19]](https://arxiv.org/abs/1812.00090#:~:text=,DNAS%29%20framework) Mixed Precision Quantization of ConvNets via Differentiable Neural ...

<https://arxiv.org/abs/1812.00090>

[[21]](https://arxiv.org/html/2410.24214v1#:~:text=This%20paper%20introduces%20ARQ%2C%20an,better%20than%20these%20baselines%20across) [[22]](https://arxiv.org/html/2410.24214v1#:~:text=that%20not%20only%20preserves%20the,In%20many%20cases%2C%20the) [[23]](https://arxiv.org/html/2410.24214v1#:~:text=maintains%20their%20certified%20robustness,of%20the%20original%20DNN%20with) [[24]](https://arxiv.org/html/2410.24214v1#:~:text=quantization%20policies%20%E2%80%93%20the%20bit,grained%20control%20over%20the) [[25]](https://arxiv.org/html/2410.24214v1#:~:text=quantization%20methods%20that%20only%20optimize,ImageNet%2C%20and%20MobileNetV2%20on%20ImageNet) ARQ: A Mixed-Precision Quantization Framework for Accurate and Certifiably Robust DNNs

<https://arxiv.org/html/2410.24214v1>

[[26]](https://arxiv.org/abs/2203.08368#:~:text=layer,we%20propose%20a%20joint%20training) [[27]](https://arxiv.org/abs/2203.08368#:~:text=reveal%20that%20some%20unique%20learnable,parallelizing%20the%20original%20sequential%20training) [[28]](https://arxiv.org/abs/2203.08368#:~:text=contains%20hundreds%20of%20such%20indicators%2C,on%20ResNet18%20with%20our%20indicators) [[29]](https://arxiv.org/abs/2203.08368#:~:text=scheme%20that%20can%20obtain%20all,ranging%20models%20with%20various) [[30]](https://arxiv.org/abs/2203.08368#:~:text=processes,available%20on%20this%20https%20URL) [2203.08368] Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance

<https://arxiv.org/abs/2203.08368>

[[34]](https://arxiv.org/abs/2307.05657#:~:text=based%20MPQ%20algorithm%20that%20captures,available%20here%3A%20this%20https%20URL) [2307.05657] Mixed-Precision Quantization for Deep Vision Models with Integer Quadratic Programming

<https://arxiv.org/abs/2307.05657>

[[36]](https://openreview.net/forum?id=POWv6hDd9XH#:~:text=PTQ%20framework%2C%20dubbed%20BRECQ%2C%20which,architectures%20are%20conducted%20for%20both) [[37]](https://openreview.net/forum?id=POWv6hDd9XH#:~:text=requires%20a%20small%20subset%20of,layer%20sensitivity.%20Extensive%20experiments) [[38]](https://openreview.net/forum?id=POWv6hDd9XH#:~:text=image%20classification%20and%20object%20detection,com%2Fyhhhli%2FBRECQ) [[39]](https://openreview.net/forum?id=POWv6hDd9XH#:~:text=networks%20and%20reconstructs%20them%20one,240%20times%20faster%20production%20of) BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction | OpenReview

<https://openreview.net/forum?id=POWv6hDd9XH>

[[40]](https://proceedings.mlr.press/v238/zhao24d/zhao24d.pdf#:~:text=%5BPDF%5D%20Self,manner%20without%20labeled%20data) [[41]](https://proceedings.mlr.press/v238/zhao24d/zhao24d.pdf#:~:text=Second%2C%20we%20propose%20a%20Self,manner%20without%20labeled%20data) [PDF] Self-Supervised Quantization-Aware Knowledge Distillation

<https://proceedings.mlr.press/v238/zhao24d/zhao24d.pdf>

[[42]](https://www.marktechpost.com/2022/07/18/meet-intel-neural-compressor-an-open-source-python-library-for-model-compression-that-reduces-the-model-size-and-increases-the-speed-of-deep-learning-inference-for-deployment-on-cpus-or-gpus/#:~:text=In%20order%20to%20allow%20fine,The%20tool%20searches%20the) [[43]](https://www.marktechpost.com/2022/07/18/meet-intel-neural-compressor-an-open-source-python-library-for-model-compression-that-reduces-the-model-size-and-increases-the-speed-of-deep-learning-inference-for-deployment-on-cpus-or-gpus/#:~:text=adjustment%20method%20for%20increased%20quantization,reaches%20the%20accuracy%20objective%20and) [[44]](https://www.marktechpost.com/2022/07/18/meet-intel-neural-compressor-an-open-source-python-library-for-model-compression-that-reduces-the-model-size-and-increases-the-speed-of-deep-learning-inference-for-deployment-on-cpus-or-gpus/#:~:text=learning%20frameworks,including%20TensorFlow%2C%20PyTorch%2C%20and%20MXNet) [[45]](https://www.marktechpost.com/2022/07/18/meet-intel-neural-compressor-an-open-source-python-library-for-model-compression-that-reduces-the-model-size-and-increases-the-speed-of-deep-learning-inference-for-deployment-on-cpus-or-gpus/#:~:text=quantization%2C%20automatic%20mixed%20precision%2C%20and,tuning%20space%20of%20various%20quantization) Meet Intel® Neural Compressor: An Open-Source Python Library for Model Compression that Reduces the Model Size and Increases the Speed of Deep Learning Inference for Deployment on CPUs or GPUs - MarkTechPost

<https://www.marktechpost.com/2022/07/18/meet-intel-neural-compressor-an-open-source-python-library-for-model-compression-that-reduces-the-model-size-and-increases-the-speed-of-deep-learning-inference-for-deployment-on-cpus-or-gpus/>

[[46]](https://medium.com/openvino-toolkit/enhanced-low-precision-pipeline-to-accelerate-inference-with-openvino-toolkit-deefe0206c24#:~:text=,while%20maintaining%20a%20predefined) Enhanced Low-Precision Pipeline to Accelerate Inference with ...

<https://medium.com/openvino-toolkit/enhanced-low-precision-pipeline-to-accelerate-inference-with-openvino-toolkit-deefe0206c24>

[[47]](https://github.com/mit-han-lab/haq#:~:text=HAQ%3A%20Hardware,CVPR2019%2C%20oral) HAQ: Hardware-Aware Automated Quantization with Mixed Precision

<https://github.com/mit-han-lab/haq>

[[48]](https://github.com/Zhen-Dong/HAWQ#:~:text=Zhen,V3) Zhen-Dong/HAWQ: Quantization library for PyTorch ... - GitHub

<https://github.com/Zhen-Dong/HAWQ>
