{
  "scheme": {
    "name": "wfp8_afp16_autoquant_lm",
    "auto_quantize_bits": 8.0,
    "auto_quantize_method": "gradient",
    "auto_quantize_score_size": 128,
    "coverage_mode": "lm_only",
    "coverage_fraction": 1.0,
    "quant_formats": [
      "FP8_WEIGHT_ONLY_CFG"
    ]
  },
  "model": {
    "id": "/workspace/code/auto-quantize-model/models/qwen3_vl_4b_instruct/checkpoints/Qwen3-VL-4B-Instruct"
  },
  "num_quantized_layers": 252,
  "layers": {
    "layers.0.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.0.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.0.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.0.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.0.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.0.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.0.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.1.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.1.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.1.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.1.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.1.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.1.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.1.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.2.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.2.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.2.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.2.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.2.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.2.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.2.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.3.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.3.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.3.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.3.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.3.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.3.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.3.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.4.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.4.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.4.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.4.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.4.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.4.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.4.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.5.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.5.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.5.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.5.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.5.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.5.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.5.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.6.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.6.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.6.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.6.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.6.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.6.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.6.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.7.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.7.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.7.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.7.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.7.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.7.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.7.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.8.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.8.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.8.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.8.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.8.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.8.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.8.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.9.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.9.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.9.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.9.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.9.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.9.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.9.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.10.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.10.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.10.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.10.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.10.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.10.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.10.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.11.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.11.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.11.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.11.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.11.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.11.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.11.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.12.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.12.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.12.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.12.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.12.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.12.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.12.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.13.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.13.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.13.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.13.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.13.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.13.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.13.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.14.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.14.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.14.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.14.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.14.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.14.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.14.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.15.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.15.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.15.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.15.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.15.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.15.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.15.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.16.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.16.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.16.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.16.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.16.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.16.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.16.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.17.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.17.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.17.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.17.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.17.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.17.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.17.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.18.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.18.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.18.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.18.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.18.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.18.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.18.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.19.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.19.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.19.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.19.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.19.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.19.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.19.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.20.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.20.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.20.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.20.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.20.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.20.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.20.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.21.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.21.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.21.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.21.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.21.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.21.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.21.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.22.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.22.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.22.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.22.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.22.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.22.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.22.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.23.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.23.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.23.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.23.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.23.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.23.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.23.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.24.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.24.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.24.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.24.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.24.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.24.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.24.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.25.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.25.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.25.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.25.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.25.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.25.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.25.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.26.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.26.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.26.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.26.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.26.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.26.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.26.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.27.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.27.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.27.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.27.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.27.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.27.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.27.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.28.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.28.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.28.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.28.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.28.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.28.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.28.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.29.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.29.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.29.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.29.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.29.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.29.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.29.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.30.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.30.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.30.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.30.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.30.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.30.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.30.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.31.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.31.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.31.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.31.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.31.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.31.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.31.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.32.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.32.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.32.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.32.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.32.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.32.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.32.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.33.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.33.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.33.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.33.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.33.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.33.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.33.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.34.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.34.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.34.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.34.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.34.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.34.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.34.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.35.self_attn.q_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.35.self_attn.k_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.35.self_attn.v_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.35.self_attn.o_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.35.mlp.gate_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.35.mlp.up_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    },
    "layers.35.mlp.down_proj": {
      "quantized": true,
      "module_type": "QuantLinear"
    }
  },
  "autoquant_state": {
    "keys": [
      "candidate_stats",
      "best",
      "constraints"
    ],
    "constraints": {
      "effective_bits": 8.0
    },
    "score": 0.2731442424892343,
    "is_satisfied": true
  },
  "layer_sensitivity": {
    "layers.0.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        5.679434593730548e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 5.679434593730548e-06,
      "rank": 72
    },
    "layers.0.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.3254032182885567e-06,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.3254032182885567e-06,
      "rank": 59
    },
    "layers.0.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.007328823092393577,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.007328823092393577,
      "rank": 134
    },
    "layers.0.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.03230519779026508,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.03230519779026508,
      "rank": 144
    },
    "layers.1.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        6.07043919131911e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 6.07043919131911e-07,
      "rank": 37
    },
    "layers.1.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        4.3419876760708576e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 4.3419876760708576e-07,
      "rank": 26
    },
    "layers.1.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.004806741955690086,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.004806741955690086,
      "rank": 123
    },
    "layers.1.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0026601222343742847,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0026601222343742847,
      "rank": 111
    },
    "layers.2.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        4.6679449638986625e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 4.6679449638986625e-07,
      "rank": 30
    },
    "layers.2.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.577252461970602e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.577252461970602e-07,
      "rank": 3
    },
    "layers.2.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.009611511486582458,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.009611511486582458,
      "rank": 139
    },
    "layers.2.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0032779647735878825,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0032779647735878825,
      "rank": 113
    },
    "layers.3.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        8.724816566996196e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 8.724816566996196e-07,
      "rank": 44
    },
    "layers.3.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.9241480231357855e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.9241480231357855e-07,
      "rank": 7
    },
    "layers.3.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.014469000278040767,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.014469000278040767,
      "rank": 142
    },
    "layers.3.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.004748699022457004,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.004748699022457004,
      "rank": 122
    },
    "layers.4.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.0210357288542582e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.0210357288542582e-06,
      "rank": 51
    },
    "layers.4.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        3.106885628767486e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 3.106885628767486e-07,
      "rank": 20
    },
    "layers.4.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.021265293937176466,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.021265293937176466,
      "rank": 143
    },
    "layers.4.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.008907605893909931,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.008907605893909931,
      "rank": 137
    },
    "layers.5.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.2159214506368698e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.2159214506368698e-06,
      "rank": 56
    },
    "layers.5.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        3.866403943675323e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 3.866403943675323e-07,
      "rank": 22
    },
    "layers.5.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.014191708527505398,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.014191708527505398,
      "rank": 141
    },
    "layers.5.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0075878663919866085,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0075878663919866085,
      "rank": 136
    },
    "layers.6.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.6546202061817894e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.6546202061817894e-06,
      "rank": 64
    },
    "layers.6.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.2295041642573779e-06,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.2295041642573779e-06,
      "rank": 57
    },
    "layers.6.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.010804744204506278,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.010804744204506278,
      "rank": 140
    },
    "layers.6.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00499964295886457,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.00499964295886457,
      "rank": 126
    },
    "layers.7.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.329186756355739e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 2.329186756355739e-06,
      "rank": 69
    },
    "layers.7.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        4.502867625433282e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 4.502867625433282e-07,
      "rank": 28
    },
    "layers.7.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.009444758179597557,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.009444758179597557,
      "rank": 138
    },
    "layers.7.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0037767011672258377,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0037767011672258377,
      "rank": 117
    },
    "layers.8.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.07716206546138e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 2.07716206546138e-06,
      "rank": 66
    },
    "layers.8.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        6.927388369604159e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 6.927388369604159e-07,
      "rank": 38
    },
    "layers.8.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.007109029334969819,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.007109029334969819,
      "rank": 133
    },
    "layers.8.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0034854639088734984,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0034854639088734984,
      "rank": 115
    },
    "layers.9.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.374707463559389e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 2.374707463559389e-06,
      "rank": 71
    },
    "layers.9.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        4.3831720120124373e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 4.3831720120124373e-07,
      "rank": 27
    },
    "layers.9.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.006600882159546018,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.006600882159546018,
      "rank": 128
    },
    "layers.9.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.004353243857622147,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.004353243857622147,
      "rank": 118
    },
    "layers.10.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.3442218832769868e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 2.3442218832769868e-06,
      "rank": 70
    },
    "layers.10.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        5.237732239038451e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 5.237732239038451e-07,
      "rank": 35
    },
    "layers.10.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.007061356329359114,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.007061356329359114,
      "rank": 132
    },
    "layers.10.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00461588567122817,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.00461588567122817,
      "rank": 120
    },
    "layers.11.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.1516034962587582e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 2.1516034962587582e-06,
      "rank": 67
    },
    "layers.11.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        4.188094493429162e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 4.188094493429162e-07,
      "rank": 25
    },
    "layers.11.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0054664399940520525,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0054664399940520525,
      "rank": 127
    },
    "layers.11.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00332399969920516,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.00332399969920516,
      "rank": 114
    },
    "layers.12.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        8.274134728480931e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 8.274134728480931e-07,
      "rank": 42
    },
    "layers.12.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        4.73199193606888e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 4.73199193606888e-07,
      "rank": 32
    },
    "layers.12.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.006667408044449985,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.006667408044449985,
      "rank": 129
    },
    "layers.12.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.004462657729163766,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.004462657729163766,
      "rank": 119
    },
    "layers.13.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.380712703280551e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.380712703280551e-06,
      "rank": 60
    },
    "layers.13.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.890184447323918e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 2.890184447323918e-07,
      "rank": 18
    },
    "layers.13.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.006894759833812714,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.006894759833812714,
      "rank": 131
    },
    "layers.13.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00486316648311913,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.00486316648311913,
      "rank": 125
    },
    "layers.14.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.625096494706213e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.625096494706213e-06,
      "rank": 62
    },
    "layers.14.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        4.5149980110181787e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 4.5149980110181787e-07,
      "rank": 29
    },
    "layers.14.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.007561314618214965,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.007561314618214965,
      "rank": 135
    },
    "layers.14.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.004619247280061245,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.004619247280061245,
      "rank": 121
    },
    "layers.15.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        9.384690571323517e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 9.384690571323517e-07,
      "rank": 48
    },
    "layers.15.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        4.01396547999866e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 4.01396547999866e-07,
      "rank": 24
    },
    "layers.15.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0068468209356069565,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0068468209356069565,
      "rank": 130
    },
    "layers.15.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0037365444004535675,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0037365444004535675,
      "rank": 116
    },
    "layers.16.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.1951089149420113e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.1951089149420113e-06,
      "rank": 55
    },
    "layers.16.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.974313701997744e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 2.974313701997744e-07,
      "rank": 19
    },
    "layers.16.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.004851424600929022,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.004851424600929022,
      "rank": 124
    },
    "layers.16.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0022797163110226393,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0022797163110226393,
      "rank": 110
    },
    "layers.17.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.3064335036006014e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.3064335036006014e-06,
      "rank": 58
    },
    "layers.17.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.4021077393854284e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 2.4021077393854284e-07,
      "rank": 14
    },
    "layers.17.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0030812237528152764,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0030812237528152764,
      "rank": 112
    },
    "layers.17.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0014016791828908026,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0014016791828908026,
      "rank": 107
    },
    "layers.18.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        9.028881429173907e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 9.028881429173907e-07,
      "rank": 46
    },
    "layers.18.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.651351778126809e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 2.651351778126809e-07,
      "rank": 17
    },
    "layers.18.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0022356617264449596,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0022356617264449596,
      "rank": 109
    },
    "layers.18.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0012667565024457872,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0012667565024457872,
      "rank": 106
    },
    "layers.19.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        7.621682840408539e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 7.621682840408539e-07,
      "rank": 39
    },
    "layers.19.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.4118896391200906e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 2.4118896391200906e-07,
      "rank": 15
    },
    "layers.19.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0017644533072598279,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0017644533072598279,
      "rank": 108
    },
    "layers.19.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0007887835381552577,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0007887835381552577,
      "rank": 103
    },
    "layers.20.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        5.788263770512003e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 5.788263770512003e-07,
      "rank": 36
    },
    "layers.20.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.090175357238877e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 2.090175357238877e-07,
      "rank": 11
    },
    "layers.20.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0009543588239466771,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0009543588239466771,
      "rank": 105
    },
    "layers.20.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0004889822885161266,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0004889822885161266,
      "rank": 98
    },
    "layers.21.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.159667573664592e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 2.159667573664592e-06,
      "rank": 68
    },
    "layers.21.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.3483320887862646e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 2.3483320887862646e-07,
      "rank": 13
    },
    "layers.21.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0008420550293521956,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0008420550293521956,
      "rank": 104
    },
    "layers.21.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00045518648403231055,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.00045518648403231055,
      "rank": 97
    },
    "layers.22.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.7298565921919362e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.7298565921919362e-06,
      "rank": 65
    },
    "layers.22.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        3.976104210323683e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 3.976104210323683e-07,
      "rank": 23
    },
    "layers.22.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0007028110412647948,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0007028110412647948,
      "rank": 102
    },
    "layers.22.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0005475208745338023,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.0005475208745338023,
      "rank": 100
    },
    "layers.23.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.6504493771662965e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.6504493771662965e-06,
      "rank": 63
    },
    "layers.23.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        5.231863156041072e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 5.231863156041072e-07,
      "rank": 34
    },
    "layers.23.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0005899031966691837,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0005899031966691837,
      "rank": 101
    },
    "layers.23.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00032818839827086776,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.00032818839827086776,
      "rank": 95
    },
    "layers.24.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.3892837102957856e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.3892837102957856e-06,
      "rank": 61
    },
    "layers.24.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        3.3039216873476107e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 3.3039216873476107e-07,
      "rank": 21
    },
    "layers.24.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0005039035168010741,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0005039035168010741,
      "rank": 99
    },
    "layers.24.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00023587355099152774,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.00023587355099152774,
      "rank": 93
    },
    "layers.25.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.073148681030034e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.073148681030034e-06,
      "rank": 52
    },
    "layers.25.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.9256988537108555e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.9256988537108555e-07,
      "rank": 9
    },
    "layers.25.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00036940468999091536,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.00036940468999091536,
      "rank": 96
    },
    "layers.25.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00019157792121404782,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.00019157792121404782,
      "rank": 92
    },
    "layers.26.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.0905402945127207e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.0905402945127207e-06,
      "rank": 53
    },
    "layers.26.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.9253874938840454e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.9253874938840454e-07,
      "rank": 8
    },
    "layers.26.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.0002743990335147828,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.0002743990335147828,
      "rank": 94
    },
    "layers.26.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00013672486966243014,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 0.00013672486966243014,
      "rank": 90
    },
    "layers.27.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        7.855820811641934e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 7.855820811641934e-07,
      "rank": 40
    },
    "layers.27.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.9499719883242506e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.9499719883242506e-07,
      "rank": 10
    },
    "layers.27.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00018991697288583964,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.00018991697288583964,
      "rank": 91
    },
    "layers.27.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        8.66210539243184e-05,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 8.66210539243184e-05,
      "rank": 88
    },
    "layers.28.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        9.939255107838108e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 9.939255107838108e-07,
      "rank": 49
    },
    "layers.28.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.3387780601069608e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 2.3387780601069608e-07,
      "rank": 12
    },
    "layers.28.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        0.00012034904102620203,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 0.00012034904102620203,
      "rank": 89
    },
    "layers.28.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        5.386085467762314e-05,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 5.386085467762314e-05,
      "rank": 84
    },
    "layers.29.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        8.263965654009553e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 8.263965654009553e-07,
      "rank": 41
    },
    "layers.29.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.6709484640387018e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.6709484640387018e-07,
      "rank": 4
    },
    "layers.29.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        7.920109874248737e-05,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 7.920109874248737e-05,
      "rank": 87
    },
    "layers.29.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        3.4038001103908755e-05,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 3.4038001103908755e-05,
      "rank": 82
    },
    "layers.30.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.0113289619084753e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.0113289619084753e-06,
      "rank": 50
    },
    "layers.30.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.7659664308666834e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.7659664308666834e-07,
      "rank": 5
    },
    "layers.30.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        5.739414700656198e-05,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 5.739414700656198e-05,
      "rank": 85
    },
    "layers.30.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.6249717848259024e-05,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 2.6249717848259024e-05,
      "rank": 79
    },
    "layers.31.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        8.543985714482005e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 8.543985714482005e-07,
      "rank": 43
    },
    "layers.31.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.770260169564608e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.770260169564608e-07,
      "rank": 6
    },
    "layers.31.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        4.625826022675028e-05,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 4.625826022675028e-05,
      "rank": 83
    },
    "layers.31.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.2807975256000645e-05,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 2.2807975256000645e-05,
      "rank": 75
    },
    "layers.32.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        8.783978877602294e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 8.783978877602294e-07,
      "rank": 45
    },
    "layers.32.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.362338011290376e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 1.362338011290376e-07,
      "rank": 2
    },
    "layers.32.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        3.3605745102249784e-05,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 3.3605745102249784e-05,
      "rank": 81
    },
    "layers.32.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        3.257993648730917e-05,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 3.257993648730917e-05,
      "rank": 80
    },
    "layers.33.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        5.225810753017868e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 5.225810753017868e-07,
      "rank": 33
    },
    "layers.33.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        8.626292569147154e-08,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 8.626292569147154e-08,
      "rank": 1
    },
    "layers.33.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.37849667428236e-05,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 2.37849667428236e-05,
      "rank": 76
    },
    "layers.33.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        8.87818214323488e-06,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 8.87818214323488e-06,
      "rank": 73
    },
    "layers.34.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        9.250259509485659e-07,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 9.250259509485659e-07,
      "rank": 47
    },
    "layers.34.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.419340034975903e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 2.419340034975903e-07,
      "rank": 16
    },
    "layers.34.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.463015516696032e-05,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 2.463015516696032e-05,
      "rank": 77
    },
    "layers.34.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        9.820824288908625e-06,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 9.820824288908625e-06,
      "rank": 74
    },
    "layers.35.self_attn.q_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        1.1617358168791725e-06,
        0.0
      ],
      "costs": [
        7864320.0,
        15728640.0
      ],
      "importance": 1.1617358168791725e-06,
      "rank": 54
    },
    "layers.35.self_attn.o_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        4.718266950476391e-07,
        0.0
      ],
      "costs": [
        5242880.0,
        10485760.0
      ],
      "importance": 4.718266950476391e-07,
      "rank": 31
    },
    "layers.35.mlp.gate_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        6.139582819741918e-05,
        0.0
      ],
      "costs": [
        24903680.0,
        49807360.0
      ],
      "importance": 6.139582819741918e-05,
      "rank": 86
    },
    "layers.35.mlp.down_proj.quant_recipe": {
      "formats": [
        "CUSTOM_0(effective-bits: 8.0)",
        "NONE(effective-bits: 16.0)"
      ],
      "scores": [
        2.5109685338975396e-05,
        0.0
      ],
      "costs": [
        12451840.0,
        24903680.0
      ],
      "importance": 2.5109685338975396e-05,
      "rank": 78
    }
  },
  "sensitivity_ranking": [
    {
      "name": "layers.33.self_attn.o_proj.quant_recipe",
      "importance": 8.626292569147154e-08
    },
    {
      "name": "layers.32.self_attn.o_proj.quant_recipe",
      "importance": 1.362338011290376e-07
    },
    {
      "name": "layers.2.self_attn.o_proj.quant_recipe",
      "importance": 1.577252461970602e-07
    },
    {
      "name": "layers.29.self_attn.o_proj.quant_recipe",
      "importance": 1.6709484640387018e-07
    },
    {
      "name": "layers.30.self_attn.o_proj.quant_recipe",
      "importance": 1.7659664308666834e-07
    },
    {
      "name": "layers.31.self_attn.o_proj.quant_recipe",
      "importance": 1.770260169564608e-07
    },
    {
      "name": "layers.3.self_attn.o_proj.quant_recipe",
      "importance": 1.9241480231357855e-07
    },
    {
      "name": "layers.26.self_attn.o_proj.quant_recipe",
      "importance": 1.9253874938840454e-07
    },
    {
      "name": "layers.25.self_attn.o_proj.quant_recipe",
      "importance": 1.9256988537108555e-07
    },
    {
      "name": "layers.27.self_attn.o_proj.quant_recipe",
      "importance": 1.9499719883242506e-07
    },
    {
      "name": "layers.20.self_attn.o_proj.quant_recipe",
      "importance": 2.090175357238877e-07
    },
    {
      "name": "layers.28.self_attn.o_proj.quant_recipe",
      "importance": 2.3387780601069608e-07
    },
    {
      "name": "layers.21.self_attn.o_proj.quant_recipe",
      "importance": 2.3483320887862646e-07
    },
    {
      "name": "layers.17.self_attn.o_proj.quant_recipe",
      "importance": 2.4021077393854284e-07
    },
    {
      "name": "layers.19.self_attn.o_proj.quant_recipe",
      "importance": 2.4118896391200906e-07
    },
    {
      "name": "layers.34.self_attn.o_proj.quant_recipe",
      "importance": 2.419340034975903e-07
    },
    {
      "name": "layers.18.self_attn.o_proj.quant_recipe",
      "importance": 2.651351778126809e-07
    },
    {
      "name": "layers.13.self_attn.o_proj.quant_recipe",
      "importance": 2.890184447323918e-07
    },
    {
      "name": "layers.16.self_attn.o_proj.quant_recipe",
      "importance": 2.974313701997744e-07
    },
    {
      "name": "layers.4.self_attn.o_proj.quant_recipe",
      "importance": 3.106885628767486e-07
    },
    {
      "name": "layers.24.self_attn.o_proj.quant_recipe",
      "importance": 3.3039216873476107e-07
    },
    {
      "name": "layers.5.self_attn.o_proj.quant_recipe",
      "importance": 3.866403943675323e-07
    },
    {
      "name": "layers.22.self_attn.o_proj.quant_recipe",
      "importance": 3.976104210323683e-07
    },
    {
      "name": "layers.15.self_attn.o_proj.quant_recipe",
      "importance": 4.01396547999866e-07
    },
    {
      "name": "layers.11.self_attn.o_proj.quant_recipe",
      "importance": 4.188094493429162e-07
    },
    {
      "name": "layers.1.self_attn.o_proj.quant_recipe",
      "importance": 4.3419876760708576e-07
    },
    {
      "name": "layers.9.self_attn.o_proj.quant_recipe",
      "importance": 4.3831720120124373e-07
    },
    {
      "name": "layers.7.self_attn.o_proj.quant_recipe",
      "importance": 4.502867625433282e-07
    },
    {
      "name": "layers.14.self_attn.o_proj.quant_recipe",
      "importance": 4.5149980110181787e-07
    },
    {
      "name": "layers.2.self_attn.q_proj.quant_recipe",
      "importance": 4.6679449638986625e-07
    },
    {
      "name": "layers.35.self_attn.o_proj.quant_recipe",
      "importance": 4.718266950476391e-07
    },
    {
      "name": "layers.12.self_attn.o_proj.quant_recipe",
      "importance": 4.73199193606888e-07
    },
    {
      "name": "layers.33.self_attn.q_proj.quant_recipe",
      "importance": 5.225810753017868e-07
    },
    {
      "name": "layers.23.self_attn.o_proj.quant_recipe",
      "importance": 5.231863156041072e-07
    },
    {
      "name": "layers.10.self_attn.o_proj.quant_recipe",
      "importance": 5.237732239038451e-07
    },
    {
      "name": "layers.20.self_attn.q_proj.quant_recipe",
      "importance": 5.788263770512003e-07
    },
    {
      "name": "layers.1.self_attn.q_proj.quant_recipe",
      "importance": 6.07043919131911e-07
    },
    {
      "name": "layers.8.self_attn.o_proj.quant_recipe",
      "importance": 6.927388369604159e-07
    },
    {
      "name": "layers.19.self_attn.q_proj.quant_recipe",
      "importance": 7.621682840408539e-07
    },
    {
      "name": "layers.27.self_attn.q_proj.quant_recipe",
      "importance": 7.855820811641934e-07
    },
    {
      "name": "layers.29.self_attn.q_proj.quant_recipe",
      "importance": 8.263965654009553e-07
    },
    {
      "name": "layers.12.self_attn.q_proj.quant_recipe",
      "importance": 8.274134728480931e-07
    },
    {
      "name": "layers.31.self_attn.q_proj.quant_recipe",
      "importance": 8.543985714482005e-07
    },
    {
      "name": "layers.3.self_attn.q_proj.quant_recipe",
      "importance": 8.724816566996196e-07
    },
    {
      "name": "layers.32.self_attn.q_proj.quant_recipe",
      "importance": 8.783978877602294e-07
    },
    {
      "name": "layers.18.self_attn.q_proj.quant_recipe",
      "importance": 9.028881429173907e-07
    },
    {
      "name": "layers.34.self_attn.q_proj.quant_recipe",
      "importance": 9.250259509485659e-07
    },
    {
      "name": "layers.15.self_attn.q_proj.quant_recipe",
      "importance": 9.384690571323517e-07
    },
    {
      "name": "layers.28.self_attn.q_proj.quant_recipe",
      "importance": 9.939255107838108e-07
    },
    {
      "name": "layers.30.self_attn.q_proj.quant_recipe",
      "importance": 1.0113289619084753e-06
    },
    {
      "name": "layers.4.self_attn.q_proj.quant_recipe",
      "importance": 1.0210357288542582e-06
    },
    {
      "name": "layers.25.self_attn.q_proj.quant_recipe",
      "importance": 1.073148681030034e-06
    },
    {
      "name": "layers.26.self_attn.q_proj.quant_recipe",
      "importance": 1.0905402945127207e-06
    },
    {
      "name": "layers.35.self_attn.q_proj.quant_recipe",
      "importance": 1.1617358168791725e-06
    },
    {
      "name": "layers.16.self_attn.q_proj.quant_recipe",
      "importance": 1.1951089149420113e-06
    },
    {
      "name": "layers.5.self_attn.q_proj.quant_recipe",
      "importance": 1.2159214506368698e-06
    },
    {
      "name": "layers.6.self_attn.o_proj.quant_recipe",
      "importance": 1.2295041642573779e-06
    },
    {
      "name": "layers.17.self_attn.q_proj.quant_recipe",
      "importance": 1.3064335036006014e-06
    },
    {
      "name": "layers.0.self_attn.o_proj.quant_recipe",
      "importance": 1.3254032182885567e-06
    },
    {
      "name": "layers.13.self_attn.q_proj.quant_recipe",
      "importance": 1.380712703280551e-06
    },
    {
      "name": "layers.24.self_attn.q_proj.quant_recipe",
      "importance": 1.3892837102957856e-06
    },
    {
      "name": "layers.14.self_attn.q_proj.quant_recipe",
      "importance": 1.625096494706213e-06
    },
    {
      "name": "layers.23.self_attn.q_proj.quant_recipe",
      "importance": 1.6504493771662965e-06
    },
    {
      "name": "layers.6.self_attn.q_proj.quant_recipe",
      "importance": 1.6546202061817894e-06
    },
    {
      "name": "layers.22.self_attn.q_proj.quant_recipe",
      "importance": 1.7298565921919362e-06
    },
    {
      "name": "layers.8.self_attn.q_proj.quant_recipe",
      "importance": 2.07716206546138e-06
    },
    {
      "name": "layers.11.self_attn.q_proj.quant_recipe",
      "importance": 2.1516034962587582e-06
    },
    {
      "name": "layers.21.self_attn.q_proj.quant_recipe",
      "importance": 2.159667573664592e-06
    },
    {
      "name": "layers.7.self_attn.q_proj.quant_recipe",
      "importance": 2.329186756355739e-06
    },
    {
      "name": "layers.10.self_attn.q_proj.quant_recipe",
      "importance": 2.3442218832769868e-06
    },
    {
      "name": "layers.9.self_attn.q_proj.quant_recipe",
      "importance": 2.374707463559389e-06
    },
    {
      "name": "layers.0.self_attn.q_proj.quant_recipe",
      "importance": 5.679434593730548e-06
    },
    {
      "name": "layers.33.mlp.down_proj.quant_recipe",
      "importance": 8.87818214323488e-06
    },
    {
      "name": "layers.34.mlp.down_proj.quant_recipe",
      "importance": 9.820824288908625e-06
    },
    {
      "name": "layers.31.mlp.down_proj.quant_recipe",
      "importance": 2.2807975256000645e-05
    },
    {
      "name": "layers.33.mlp.gate_proj.quant_recipe",
      "importance": 2.37849667428236e-05
    },
    {
      "name": "layers.34.mlp.gate_proj.quant_recipe",
      "importance": 2.463015516696032e-05
    },
    {
      "name": "layers.35.mlp.down_proj.quant_recipe",
      "importance": 2.5109685338975396e-05
    },
    {
      "name": "layers.30.mlp.down_proj.quant_recipe",
      "importance": 2.6249717848259024e-05
    },
    {
      "name": "layers.32.mlp.down_proj.quant_recipe",
      "importance": 3.257993648730917e-05
    },
    {
      "name": "layers.32.mlp.gate_proj.quant_recipe",
      "importance": 3.3605745102249784e-05
    },
    {
      "name": "layers.29.mlp.down_proj.quant_recipe",
      "importance": 3.4038001103908755e-05
    },
    {
      "name": "layers.31.mlp.gate_proj.quant_recipe",
      "importance": 4.625826022675028e-05
    },
    {
      "name": "layers.28.mlp.down_proj.quant_recipe",
      "importance": 5.386085467762314e-05
    },
    {
      "name": "layers.30.mlp.gate_proj.quant_recipe",
      "importance": 5.739414700656198e-05
    },
    {
      "name": "layers.35.mlp.gate_proj.quant_recipe",
      "importance": 6.139582819741918e-05
    },
    {
      "name": "layers.29.mlp.gate_proj.quant_recipe",
      "importance": 7.920109874248737e-05
    },
    {
      "name": "layers.27.mlp.down_proj.quant_recipe",
      "importance": 8.66210539243184e-05
    },
    {
      "name": "layers.28.mlp.gate_proj.quant_recipe",
      "importance": 0.00012034904102620203
    },
    {
      "name": "layers.26.mlp.down_proj.quant_recipe",
      "importance": 0.00013672486966243014
    },
    {
      "name": "layers.27.mlp.gate_proj.quant_recipe",
      "importance": 0.00018991697288583964
    },
    {
      "name": "layers.25.mlp.down_proj.quant_recipe",
      "importance": 0.00019157792121404782
    },
    {
      "name": "layers.24.mlp.down_proj.quant_recipe",
      "importance": 0.00023587355099152774
    },
    {
      "name": "layers.26.mlp.gate_proj.quant_recipe",
      "importance": 0.0002743990335147828
    },
    {
      "name": "layers.23.mlp.down_proj.quant_recipe",
      "importance": 0.00032818839827086776
    },
    {
      "name": "layers.25.mlp.gate_proj.quant_recipe",
      "importance": 0.00036940468999091536
    },
    {
      "name": "layers.21.mlp.down_proj.quant_recipe",
      "importance": 0.00045518648403231055
    },
    {
      "name": "layers.20.mlp.down_proj.quant_recipe",
      "importance": 0.0004889822885161266
    },
    {
      "name": "layers.24.mlp.gate_proj.quant_recipe",
      "importance": 0.0005039035168010741
    },
    {
      "name": "layers.22.mlp.down_proj.quant_recipe",
      "importance": 0.0005475208745338023
    },
    {
      "name": "layers.23.mlp.gate_proj.quant_recipe",
      "importance": 0.0005899031966691837
    },
    {
      "name": "layers.22.mlp.gate_proj.quant_recipe",
      "importance": 0.0007028110412647948
    },
    {
      "name": "layers.19.mlp.down_proj.quant_recipe",
      "importance": 0.0007887835381552577
    },
    {
      "name": "layers.21.mlp.gate_proj.quant_recipe",
      "importance": 0.0008420550293521956
    },
    {
      "name": "layers.20.mlp.gate_proj.quant_recipe",
      "importance": 0.0009543588239466771
    },
    {
      "name": "layers.18.mlp.down_proj.quant_recipe",
      "importance": 0.0012667565024457872
    },
    {
      "name": "layers.17.mlp.down_proj.quant_recipe",
      "importance": 0.0014016791828908026
    },
    {
      "name": "layers.19.mlp.gate_proj.quant_recipe",
      "importance": 0.0017644533072598279
    },
    {
      "name": "layers.18.mlp.gate_proj.quant_recipe",
      "importance": 0.0022356617264449596
    },
    {
      "name": "layers.16.mlp.down_proj.quant_recipe",
      "importance": 0.0022797163110226393
    },
    {
      "name": "layers.1.mlp.down_proj.quant_recipe",
      "importance": 0.0026601222343742847
    },
    {
      "name": "layers.17.mlp.gate_proj.quant_recipe",
      "importance": 0.0030812237528152764
    },
    {
      "name": "layers.2.mlp.down_proj.quant_recipe",
      "importance": 0.0032779647735878825
    },
    {
      "name": "layers.11.mlp.down_proj.quant_recipe",
      "importance": 0.00332399969920516
    },
    {
      "name": "layers.8.mlp.down_proj.quant_recipe",
      "importance": 0.0034854639088734984
    },
    {
      "name": "layers.15.mlp.down_proj.quant_recipe",
      "importance": 0.0037365444004535675
    },
    {
      "name": "layers.7.mlp.down_proj.quant_recipe",
      "importance": 0.0037767011672258377
    },
    {
      "name": "layers.9.mlp.down_proj.quant_recipe",
      "importance": 0.004353243857622147
    },
    {
      "name": "layers.12.mlp.down_proj.quant_recipe",
      "importance": 0.004462657729163766
    },
    {
      "name": "layers.10.mlp.down_proj.quant_recipe",
      "importance": 0.00461588567122817
    },
    {
      "name": "layers.14.mlp.down_proj.quant_recipe",
      "importance": 0.004619247280061245
    },
    {
      "name": "layers.3.mlp.down_proj.quant_recipe",
      "importance": 0.004748699022457004
    },
    {
      "name": "layers.1.mlp.gate_proj.quant_recipe",
      "importance": 0.004806741955690086
    },
    {
      "name": "layers.16.mlp.gate_proj.quant_recipe",
      "importance": 0.004851424600929022
    },
    {
      "name": "layers.13.mlp.down_proj.quant_recipe",
      "importance": 0.00486316648311913
    },
    {
      "name": "layers.6.mlp.down_proj.quant_recipe",
      "importance": 0.00499964295886457
    },
    {
      "name": "layers.11.mlp.gate_proj.quant_recipe",
      "importance": 0.0054664399940520525
    },
    {
      "name": "layers.9.mlp.gate_proj.quant_recipe",
      "importance": 0.006600882159546018
    },
    {
      "name": "layers.12.mlp.gate_proj.quant_recipe",
      "importance": 0.006667408044449985
    },
    {
      "name": "layers.15.mlp.gate_proj.quant_recipe",
      "importance": 0.0068468209356069565
    },
    {
      "name": "layers.13.mlp.gate_proj.quant_recipe",
      "importance": 0.006894759833812714
    },
    {
      "name": "layers.10.mlp.gate_proj.quant_recipe",
      "importance": 0.007061356329359114
    },
    {
      "name": "layers.8.mlp.gate_proj.quant_recipe",
      "importance": 0.007109029334969819
    },
    {
      "name": "layers.0.mlp.gate_proj.quant_recipe",
      "importance": 0.007328823092393577
    },
    {
      "name": "layers.14.mlp.gate_proj.quant_recipe",
      "importance": 0.007561314618214965
    },
    {
      "name": "layers.5.mlp.down_proj.quant_recipe",
      "importance": 0.0075878663919866085
    },
    {
      "name": "layers.4.mlp.down_proj.quant_recipe",
      "importance": 0.008907605893909931
    },
    {
      "name": "layers.7.mlp.gate_proj.quant_recipe",
      "importance": 0.009444758179597557
    },
    {
      "name": "layers.2.mlp.gate_proj.quant_recipe",
      "importance": 0.009611511486582458
    },
    {
      "name": "layers.6.mlp.gate_proj.quant_recipe",
      "importance": 0.010804744204506278
    },
    {
      "name": "layers.5.mlp.gate_proj.quant_recipe",
      "importance": 0.014191708527505398
    },
    {
      "name": "layers.3.mlp.gate_proj.quant_recipe",
      "importance": 0.014469000278040767
    },
    {
      "name": "layers.4.mlp.gate_proj.quant_recipe",
      "importance": 0.021265293937176466
    },
    {
      "name": "layers.0.mlp.down_proj.quant_recipe",
      "importance": 0.03230519779026508
    }
  ]
}