[project]
authors = [{name = "igamenovoer", email = "igamenovoer@xx.com"}]
dependencies = [
  # Keep ONNX compatible with TensorRT-LLM 1.0.0
  # (uses onnx.helper.float32_to_bfloat16, present in 1.19.x but not 1.20.x).
  "onnx>=1.19.0,<1.20.0",
  "onnx-graphsurgeon>=0.5.8,<0.6",
  "mdutils>=1.8.1,<2",
  "imageio>=2.37.2,<3",
  "polygraphy>=0.49.26,<0.50",
  "modelscope>=1.32.0,<2",
  "transformers",
  "accelerate", "torchmetrics>=1.8.2,<2",
]
name = "auto-quantize-model"
requires-python = ">= 3.11, < 3.13"
version = "0.1.0"

[build-system]
build-backend = "hatchling.build"
requires = ["hatchling"]

[tool.pixi.workspace]
channels = ["conda-forge", "nvidia"]
platforms = ["linux-64"]

[tool.pixi.pypi-dependencies]
auto_quantize_model = { path = ".", editable = true }
ultralytics = "*"
transformers = "*"
accelerate = "*"
diffusers = "*"
safetensors = "*"
qwen-vl-utils = { version = "*", extras = ["decord"] }
requests = "*"
datasets = "*"
addict = "*"
pip = "*"
setuptools-scm = "*"
cmake = ">=3.26.1"
supervision = "*"
scipy = "*"
opencv-python-headless = "*"
imageio = "*"
attrs = "*"
omegaconf = "*"
hydra-core = "*"
mdutils = "*"
mkdocs-material = "*"
ruff = "*"
mypy = "*"
pytest = "*"
click = "*"
rich = "*"
pandas = "*"
huggingface-hub = "*"
py-cpuinfo = "*"
onnxslim = ">=0.1.76"
cppimport = "*"
mako = "*"
pybind11 = "*"
ml-dtypes = "*"
onnxscript = "*"
pycocotools = "*"

[tool.pixi.activation.env]
CUDA_HOME = "${CONDA_PREFIX}"
CUDA_PATH = "${CONDA_PREFIX}"
LD_LIBRARY_PATH = "${CONDA_PREFIX}/lib:${CONDA_PREFIX}/lib64:/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}"
PATH = "${CONDA_PREFIX}/bin:/opt/tensorrt/bin:${PATH}"
NVTOOLSEXT_PATH = "${CONDA_PREFIX}/lib/python3.12/site-packages/nvidia/nvtx"

[tool.pixi.tasks]
setup-tensorrt = "env NVIDIA_TENSORRT_DISABLE_INTERNAL_PIP=0 python -m pip install tensorrt==10.7.0 --extra-index-url https://pypi.nvidia.com"
build-vllm-wheel = "env VLLM_TAG=v0.8.5 bash extern/build-vllm.sh -o tmp/vllm-build --jobs 12 --nvcc-threads 3"
build-vllm-wheel-rtx5090 = "env VLLM_TAG=autoq-vllm-v0.10.1 TORCH_CUDA_ARCH_LIST=12.0 bash extern/build-vllm.sh -o tmp/vllm-build --jobs 12 --nvcc-threads 3"
build-onnxoptimizer-rtx5090 = "bash extern/build-onnxoptimizer-rtx5090.sh"
install-vllm-wheel = "bash custom-build/bootstrap.sh --yes && python -m pip install --no-deps custom-build/vllm-*.whl"
postinstall-vllm-rtx5090 = "python -m pip install --no-deps custom-build/vllm-0.10.2.dev2+g926b2b1d9.d20251208-cp312-cp312-linux_x86_64.whl"
postinstall-tensorrt-llm-rtx5090 = "python -m pip install --pre tensorrt_llm --extra-index-url https://pypi.nvidia.com --extra-index-url https://download.pytorch.org/whl/nightly/cu128"

[tool.pixi.dependencies]
python = "3.12.*"
#
# GPU-specific conda dependencies are attached to per-environment features
# (see [tool.pixi.feature.cu124] and [tool.pixi.feature.rtx5090]).

[tool.pixi.feature.cu124.dependencies]
cuda-toolkit = { version = "12.4.0.*", channel = "conda-forge" }
cuda-nvtx-dev = ">=12.4.99,<13"

[tool.pixi.feature.cu124.pypi-dependencies]
torch = "==2.6.0"
torchvision = "==0.21.0"
torchaudio = "==2.6.0"
onnxruntime-gpu = "*"
cupy-cuda12x = "*"
nvidia-modelopt = "==0.40.0"

[tool.pixi.feature.cu124.pypi-options]
index-url = "https://download.pytorch.org/whl/cu124"
# Primary index is the CUDA 12.4 PyTorch wheel index; fall back to PyPI for
# all other packages.
extra-index-urls = ["https://pypi.org/simple"]
index-strategy = "unsafe-best-match"

[tool.pixi.feature.rtx5090.dependencies]
# RTX 5090 CUDA toolchain pin:
# - These versions are intentionally fixed for this env.
# - Do NOT change them (including CUDA minor version) unless a human
#   explicitly requests it and understands the impact on vLLM/torch builds.
cuda-toolkit = { version = "12.8.1.*", channel = "conda-forge" }
cuda-nvtx-dev = ">=12.4.99,<13"

[tool.pixi.feature.rtx5090.pypi-dependencies]
# RTX 5090 PyTorch pin (non-vLLM):
# - Keep this stack for general RTX 5090 GPU / quantization work.
torch = "==2.9.0"
torchvision = "==0.24.0"
torchaudio = "==2.9.0"
# Use a locally built ONNX Runtime GPU wheel (built from extern/onnxruntime).
# See: `extern/howto-build-onnxruntime-with-pixi-cuda-12_8.md`.
onnxruntime-gpu = { path = "custom-build/onnxruntime_gpu-1.24.0-cp312-cp312-linux_x86_64.whl" }
cupy-cuda12x = "*"
nvidia-modelopt = "==0.40.0"

[tool.pixi.feature.rtx5090.pypi-options]
# Use the stable PyTorch cu128 index for this TensorRT-LLM-aligned stack,
# with PyPI as a fallback for non-torch packages.
index-url = "https://download.pytorch.org/whl/cu128"
extra-index-urls = ["https://pypi.org/simple"]
index-strategy = "unsafe-best-match"

[tool.pixi.feature.rtx5090.activation.env]
# Help CMake / FindCUDAToolkit locate headers/libs for the
# nvidia::cuda-toolkit layout used in the RTX 5090 env.
CUDAToolkit_ROOT = "${CONDA_PREFIX}/targets/x86_64-linux"
CUDA_TOOLKIT_ROOT_DIR = "${CONDA_PREFIX}/targets/x86_64-linux"
CUDA_HOME = "${CONDA_PREFIX}/targets/x86_64-linux"
CUDA_PATH = "${CONDA_PREFIX}/targets/x86_64-linux"
OPAL_PREFIX = "/opt/hpcx/ompi"
LD_LIBRARY_PATH = "/usr/local/tensorrt/targets/x86_64-linux-gnu/lib:${CONDA_PREFIX}/lib:${CONDA_PREFIX}/lib64:/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}"

[tool.pixi.feature.rtx5090-inc.dependencies]
# INC-focused overlay for RTX 5090:
# - Reuses the RTX 5090 CUDA/PyTorch stack and adds Intel Neural Compressor
#   and related tooling needed for PTQ and sensitivity analysis.

[tool.pixi.feature.rtx5090-inc.pypi-dependencies]
neural-compressor = ">=2.6,<3"
neural-compressor-pt = ">=3.6,<4"
onnxruntime-tools = ">=1.7.0,<2"
onnxscript = ">=0.5.6,<0.6"
optimum-onnx = { version = ">=0.0.3,<0.0.4", extras = ["onnxruntime"] }

[tool.pixi.feature.rtx5090-trtllm.dependencies]
# RTX 5090 TensorRT-LLM-focused environment:
# - Cloned from the original RTX 5090 env to keep a stable TRT-LLM stack.
cuda-toolkit = { version = "12.8.1.*", channel = "conda-forge" }
cuda-nvtx-dev = ">=12.4.99,<13"

[tool.pixi.feature.rtx5090-trtllm.pypi-dependencies]
# RTX 5090 TensorRT-LLM PyTorch pin:
# - Mirrors the original RTX 5090 stack so we can install TRT-LLM-specific
#   packages without affecting the base RTX 5090 environment.
torch = "==2.7.1"
torchvision = "*"
torchaudio = "*"
onnxruntime-gpu = "*"
cupy-cuda12x = "*"
tensorrt-llm = "==1.0.0"
nvidia-modelopt = "==0.33.1"

[tool.pixi.feature.rtx5090-trtllm.pypi-options]
# Use the stable PyTorch cu128 index for this TensorRT-LLM-aligned stack,
# with PyPI as a fallback for non-torch packages.
index-url = "https://download.pytorch.org/whl/cu128"
extra-index-urls = ["https://pypi.org/simple"]
index-strategy = "unsafe-best-match"

[tool.pixi.feature.rtx5090-trtllm.activation.env]
# Help CMake / FindCUDAToolkit locate headers/libs for the
# nvidia::cuda-toolkit layout used in the RTX 5090 TRT-LLM env.
CUDAToolkit_ROOT = "${CONDA_PREFIX}/targets/x86_64-linux"
CUDA_TOOLKIT_ROOT_DIR = "${CONDA_PREFIX}/targets/x86_64-linux"
CUDA_HOME = "${CONDA_PREFIX}/targets/x86_64-linux"
CUDA_PATH = "${CONDA_PREFIX}/targets/x86_64-linux"

[tool.pixi.feature.rtx5090-vllm.dependencies]
# RTX 5090 vLLM CUDA toolchain pin:
# - Cloned from the original RTX 5090 env to keep a stable vLLM stack.
cuda-toolkit = { version = "12.8.1.*", channel = "conda-forge" }
cuda-nvtx-dev = ">=12.4.99,<13"
libcudnn-dev = ">=9,<10"
libprotobuf = "*"

[tool.pixi.feature.rtx5090-vllm.pypi-dependencies]
# RTX 5090 vLLM PyTorch pin:
# - Cloned from the original RTX 5090 env to keep a stable vLLM stack.
torch = "==2.9.1"
torchvision = "==0.24.1"
torchaudio = "==2.9.1"
onnxruntime-gpu = "*"
cupy-cuda12x = "*"
nvidia-modelopt = "==0.40.0"
xformers = ">=0.0.33"
cloudpickle = "*"
blake3 = "*"
cachetools = "*"
cbor2 = "*"
compressed-tensors = "==0.10.2"
depyf = "==0.19.0"
diskcache = "==5.6.3"
einops = "*"
fastapi = { version = ">=0.115.0", extras = ["standard"] }
gguf = ">=0.13.0"
lark = "==1.2.2"
llguidance = ">=0.7.11,<0.8.0"
lm-format-enforcer = ">=0.10.11,<0.11"
mistral_common = { version = ">=1.8.2", extras = ["audio", "image"] }
msgspec = "*"
numba = "==0.61.2"
openai = ">=1.99.1"
openai-harmony = ">=0.0.3"
outlines_core = "==0.2.10"
partial-json-parser = "*"
prometheus_client = ">=0.18.0"
prometheus-fastapi-instrumentator = ">=7.0.0"
pybase64 = "*"
python-json-logger = "*"
pyzmq = ">=25.0.0"
ray = { version = ">=2.48.0", extras = ["cgraph"] }
sentencepiece = "*"
setproctitle = "*"
tiktoken = ">=0.6.0"
watchfiles = "*"
xgrammar = "==0.1.21"

[tool.pixi.feature.rtx5090-vllm.pypi-options]
# Use the PyTorch nightly cu128 index for Blackwell (sm_120) support,
# with PyPI as a fallback for non-torch packages.
index-url = "https://download.pytorch.org/whl/nightly/cu128"
extra-index-urls = ["https://pypi.org/simple"]
index-strategy = "unsafe-best-match"

[tool.pixi.feature.rtx5090-vllm.activation.env]
# Help CMake / FindCUDAToolkit locate headers/libs for the
# nvidia::cuda-toolkit layout used in the RTX 5090 vLLM env.
CUDAToolkit_ROOT = "${CONDA_PREFIX}/targets/x86_64-linux"
CUDA_TOOLKIT_ROOT_DIR = "${CONDA_PREFIX}/targets/x86_64-linux"
CUDA_HOME = "${CONDA_PREFIX}/targets/x86_64-linux"
CUDA_PATH = "${CONDA_PREFIX}/targets/x86_64-linux"

[tool.pytest.ini_options]
testpaths = ["tests"]

[tool.pixi.environments]
# Keep the existing environment behaviour as the CUDA 12.4 setup.
default = { features = ["cu124"], solve-group = "cu124" }
cu124 = { features = ["cu124"], solve-group = "cu124" }
# RTX 5090-specific environment using nightly PyTorch/cu128.
rtx5090 = { features = ["rtx5090"], solve-group = "rtx5090" }
rtx5090-vllm = { features = ["rtx5090-vllm"], solve-group = "rtx5090-vllm" }
rtx5090-trtllm = { features = ["rtx5090-trtllm"], solve-group = "rtx5090-trtllm" }
rtx5090-inc = { features = ["rtx5090", "rtx5090-inc"], solve-group = "rtx5090-inc" }

[dependency-groups]
rtx5090-inc = ["neural-compressor>=2.6,<3"]
rtx5090 = ["onnx>=1.19.0,<1.20.0", "brevitas>=0.12.1,<0.13", "lightning>=2.6.0,<3", "tensorboardx>=2.6.4,<3", "tensorboard>=2.20.0,<3"]
